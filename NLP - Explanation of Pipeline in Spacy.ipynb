{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing Text\n",
    "\n",
    "**When you call nlp on a text, spacy will tokenize it and then it will call each component on the doc, in order. It then returns the processed doc that you can work with**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This is raw text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the above code, when we do doc = nlp(\"text\"), the spacy will perform tokenization of each word.\n",
    "- In an nlp pipeline, tokenization is followed by tagging, parsing, ner.. then the final doc is compiled again.\n",
    "- If needed, spacy would perform all the above functions on the doc as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When processing large volumes of text, the statistical models are usually more efficient if you let them work on batches of texts. spacy's nlp.pipe method takes and iterable of texts and yields processed Doc objects. The batching is done internally.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\"This is a raw text\", \"There is a lot of text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(nlp.pipe(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Because we use pipeline, alot of processing happens in parallel as text data is divided into batches.\n",
    "\n",
    "**Tip for efficient processing**\n",
    "\n",
    "- Only apply the pipeline components you need. Getting predictions from the model that you dont actually need adds up and becomes very inefficient at scale. To prevent this, use the disable keyword argument to disable components you dont need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('$9.4 million', 'MONEY'), ('the prior year', 'DATE'), ('$2.7 million', 'MONEY')]\n",
      "\n",
      "[('twelve billion dollars', 'MONEY'), ('1b', 'MONEY')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In this eg, we only need entities of docs. so we disable other components\n",
    "\n",
    "import spacy\n",
    "\n",
    "text = [\"Net income was $9.4 million compared to the prior year of $2.7 million.\", \"Revenue exceeded twelve billion dollars, with a loss of $1b.\"]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "docs = nlp.pipe(text, disable = [\"tagger\", \"parser\"])\n",
    "for doc in docs:\n",
    "    print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neither nltk nor spacy is 100% accurate. This is because, text data is very vast and meaning of the word changes in different context. Hence spacy is not able to extract correct information all the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When you load a model, Spacy first consults the model's meta.json.\n",
    "\n",
    "these are the meta data which is loaded in the form of meta.json\n",
    "\n",
    "{\n",
    "    \n",
    "    \"lang\": \"en\"\n",
    "    \n",
    "    \"name\": \"core_web_sm\"\n",
    "    \n",
    "    \"description\": \"Example model for spaCy\"\n",
    "    \n",
    "    \"pipeline\": [\"tagger\", \"parser\", \"ner\"]\n",
    "}\n",
    "\n",
    "- Note that tokenization is always on. Hence, after tokenization, we iterate over each token to perform tagging, parsing, ner etc\n",
    "\n",
    "- Fundamentally, a spacy model consists of three components: the weights, i.e. the binary data loaded in from a dictionary, a pipeline of functions called in order (tagging, parsing, ner), and language data like the tokenization rules and annotation schemes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search for Built-in-Pipeline component online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disabling and Modifying pipeline components\n",
    "\n",
    "If you dont need a particular component of the pipeline - for example, the tagger or the parser, you can disable loading it. This can sometimes make a big difference and improve loading speed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable = [\"tagger\", \"parser\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0xc152495a58>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sometimes we want to load all the pipeline components and their weights, because you need them at different points in your application. However, if you only need a doc object with named entities, there's no need to run all pipeline components on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Apple is buying a Startup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Restoring the disabled functions\n",
    "disabled = nlp.disable_pipes(\"ner\")\n",
    "doc = nlp(\"ner is disabled now\")\n",
    "disabled.restore() # This will restore the disabled function and we can use it in future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
